# Neural Networks

Cell body -> Nevron -> Axon -> Synapses.

Perceptron

![NN](../images/SL3_Neural_Networks.png)

## Artificial Neural Networks

![NN](../images/SL3_ANN.png)

![ANN](../images/SL 3 - Neural Networks Artificial Neural Networks.png)

We need to pay attention to

* Activation function
* firing threshold

### How powerful is perceptron unit?

One activation function computes the half plane.

![perceptron](../images/SL3_perceptron_Unit.png)

What's the nice and short computing in the following?

### Boolean: AND

![perceptron_func](../images/SL3_perceptron_AND.png)

### Boolean: OR

![OR-Func](../images/SL3_perception_OR.png)

### Boolean: Not

![Unit-Not](../images/SL3_perception_NOT.png)

### XOR as Perceptron Network

![XOR](../images/SL3_perception_XOR.png)

## Perceptron Training

Given examples, find weights that map inputs to outputs.

* Perceptron rule (**threshold**)
* Gradient descent / delta rule (**un-thresholded**)

### Perceptron rules

Single Unit

The halting problem for iterations.

![perceptron-training](../images/SL3_perceptron_training.png)

## Gradient Descent

Avoid Non-linear separability issues.

![Gradient-descent](../images/SL3_Gradient_descent.png)

### Comparison of Learning rules

#### Perceptron analysis

guarantee to finite convergence only if linear separability
$$
\Delta W_i = \eta (y - y') x_i
$$
Eta = learning-rate

y = target

y' = output

#### Gradient Descent analysis

Calculus, robust, converge to local optimum

$$
\Delta W_i = \eta (y - a) x_i
$$

#### Comparing learning rules

![comparison-learning-rules](../images/SL3_compariso_of_learning_rules.png)

## Sigmoid - differentiable threshold

![Sigmoid](../images/SL3_sigmoid.png)

## Neural Network Sketch
