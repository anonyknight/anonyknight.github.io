# Neural Networks algorithms

Cell body -> Nevron -> Axon -> Synapses.

Perceptron

![NN](../images/SL3_Neural_Networks.png)

## Artificial Neural Networks

![NN](../images/SL3_ANN.png)

![ANN](../images/SL 3 - Neural Networks Artificial Neural Networks.png)

We need to pay attention to

* Activation function
* firing threshold

### How powerful is perceptron unit?

One activation function computes the half plane.

![perceptron](../images/SL3_perceptron_Unit.png)

What's the nice and short computing in the following?

### Boolean: AND

![perceptron_func](../images/SL3_perceptron_AND.png)

### Boolean: OR

![OR-Func](../images/SL3_perception_OR.png)

### Boolean: Not

![Unit-Not](../images/SL3_perception_NOT.png)

### XOR as Perceptron Network

![XOR](../images/SL3_perception_XOR.png)

## Perceptron Training

Given examples, find weights that map inputs to outputs.

* Perceptron rule (**threshold**)
* Gradient descent / delta rule (**un-thresholded**)

### Perceptron rules

Single Unit

The halting problem for iterations.

![perceptron-training](../images/SL3_perceptron_training.png)

## Gradient Descent

Avoid Non-linear separability issues.

![Gradient-descent](../images/SL3_Gradient_descent.png)

### Comparison of Learning rules

#### Perceptron rule

guarantee to finite convergence only if linear separability
$$
\Delta W_i = \eta (y - y') x_i
$$
Eta = learning-rate

y = target

y' = output

#### Gradient Descent rule

Calculus, robust, converge to local optimum

$$
\Delta W_i = \eta (y - a) x_i
$$

#### Comparing rules

![comparison-learning-rules](../images/SL3_compariso_of_learning_rules.png)

## Sigmoid - differentiable threshold

![Sigmoid](../images/SL3_sigmoid.png)

## Neural Network Sketch

Whole thing is differentiable!

Back-propagation -> computationally beneficial organization of the chain rule.

**The errors flowing backwards, sometimes it's even called error back propagation.**

Many local optimal!!!

![SL3_Neural_network_Sketch.png](../images/SL3_Neural_network_Sketch.png)

## Optimizing weights

* Gradient descent
* Advanced methods
  * Momentary
  * Higher order derivatives
  * Randomized optimization
  * Penalty for "complexity"

Optimization:: learning

* More nodes
* More layers
* Large numbers

## Restriction Bias

![SL3_Restriction_bias.png](../images/SL3_Restriction_bias.png)

## Peference Bias

Algorithm's selection of one representation over another.

What algorithm?

Gradient descent. We need to check initial weights. Normally, we pick up small random values. Local minimal variability

Low "complexity", simpler explanations.

### Occam's razor

Entities should not be multiplied unnecessarily.

## Summary

* Perceptrons - threshold unit
* Networks can produce any boolean functions.
* Perception rule - finite time for linearly separable
* General differentiable rule - back propagation & gradient descent
* Preference & restriction bias of neural networks.
